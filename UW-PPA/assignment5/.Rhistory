install.packages("digest")
load(iris)
data("iris")
str(iris)
library(swirl)
rmI(list=ls())
rm(list=ls())
sqrt(4)
swirl()
library(swirl)
swirl()
1
install.packages("UsingR")
library(UsingR)
data("father.son",package="UsingR")
str(father.son)
summary(father.son)
g <- 9.8 ##meters per second
n <- 25
tt <- seq(0,3.4,len=n) ##time in secs, t is a base function
f <- 56.67  - 0.5*g*tt^2
y <-  f + rnorm(n,sd=1)
X=cbind(1,tt,tt^2)
Beta=matrix(c(55,0,-5),3,1)
r=y- Beta%*%X
RSS=t(r)%*%r
betahat=solve(crossprod(X))%*%crossprod(X,y)
head(X)
head(y)
r=y-X%*%Beta
RSS=t(r)%*%r
betahat=solve(crossprod(X))%*%crossprod(X,y)
betahat
plot(tt,y,ylab="Distance in meters",xlab="Time in seconds")
lines(tt,f,col=2)
X <- matrix(c(1,1,1,1,0,0,1,1),nrow=4)
rownames(X) <- c("a","a","b","b")
X
beta <- c(5, 2)
X%*%beta
X <- matrix(c(1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,1,1),nrow=6)
rownames(X) <- c("a","a","b","b","c","c")
beta <- c(10,3,-3) # fitted betas
X%*%beta
g = 9.8 ## meters per second
h0 = 56.67
v0 = 0
n = 25
tt = seq(0,3.4,len=n) ##time in secs, t is a base function
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
A
A[3,3]
-2 * (A %*% y) [3]
A %*% y
numTrials<-10000
gfind <- function(N, alpha=0.01){
set.seed(1)
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
-2 * (A %*% y) [3]
}
gg <- replicate(numTrials,gfind)
mean(gg)
gg
numTrials<-10000
gfind <- function(){
set.seed(1)
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
-2 * (A %*% y) [3]
}
gg <- replicate(numTrials,gfind)
mean(gg)
gg[1]
-2 * (A %*% y) [3]
?replicate
g = 9.8 ## meters per second
h0 = 56.67
v0 = 0
n = 25
tt = seq(0,3.4,len=n) ##time in secs, t is a base function
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
-2 * (A %*% y) [3]
numTrials<-100000
gfind <- function(){
set.seed(1)
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
gval<--2 * (A %*% y) [3]
}
gg <- replicate(numTrials,gfind)
mean(gg)
numTrials<-2
gfind <- function(){
set.seed(1)
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
gval<--2 * (A %*% y) [3]
}
gg <- replicate(numTrials,gfind)
mean(gg)
numTrials<-2
gfind <- function(){
set.seed(1)
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
gval<- -2 * (A %*% y) [3]
}
gg <- replicate(numTrials,gfind)
mean(gg)
gfind
numTrials<-2
gfind <- function(){
set.seed(1)
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
gval<- -2 * (A %*% y) [3]
}
gg <- replicate(numTrials,gfind())
mean(gg)
numTrials<-100000
gfind <- function(){
set.seed(1)
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
gval<- -2 * (A %*% y) [3]
}
gg <- replicate(numTrials,gfind())
mean(gg)
?sdev
?sd
numTrials<-100000
gfind <- function(){
set.seed(1)
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
gval<- -2 * (A %*% y) [3]
}
gg <- replicate(numTrials,gfind())
mean(gg)
sd(gg)
head(gg)
numTrials<-100000
set.seed(1)
gfind <- function(){
y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1)
X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
gval<- -2 * (A %*% y) [3]
}
gg <- replicate(numTrials,gfind())
mean(gg)
sd(gg)
sqrt(sum((gg-mean(gg))^2)/numTrials
sqrt(sum((gg-mean(gg))^2))/numTrials
sqrt(sum((gg-mean(gg))^2)/numTrials)
library(UsingR)
x = father.son$fheight
y = father.son$sheight
n = length(y)
library(UsingR)
x = father.son$fheight
y = father.son$sheight
n = length(y)
numTrials<-10000
N =  50
set.seed(1)
betahats<-replicate(numTrials,
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
betahat =  lm(y~x)$coef
)
sd(betahats)
numTrials<-10000
N =  50
set.seed(1)
betahats<-replicate(numTrials,{
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
betahat =  lm(y~x)$coef
})
sd(betahats)
betahats
betahats[1]
numTrials<-2
N =  50
set.seed(1)
betahats<-replicate(numTrials,{
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
betahat =  lm(y~x)$coef
print (betahat)
})
sd(betahats)
numTrials<-2
N =  50
set.seed(1)
betahats<-replicate(numTrials,{
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
betahat =  lm(y~x)$coef
print (betahat[2])
})
sd(betahats)
numTrials<-10000
N =  50
set.seed(1)
betahats<-replicate(numTrials,{
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
betahat =  lm(y~x)$coef
#print (betahat[2])
})
sd(betahats[2])
numTrials<-10000
N =  50
set.seed(1)
betahats<-replicate(numTrials,{
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
betahat =  lm(y~x)$coef
betahat[2]
})
sd(betahats)
mean( (y - mean(y))*(x-mean(x) ) )
cov(father.son)
| $\beta$ | <span style="color:blue">_power_</span>: $1-\beta$
rnorm(1)
rnorm(100)
(.8*.01)/(.96*.99+.8*.01)
(.8*.01)/(.096*.99+.8*.01)
?summary()
?summary
?summary()
help(summary)
help(summary)
help(sum)
man(summary)
?summary()
?sum()
setwd("H:/Rspace/UW-Data-Science/UW-PPA/assignment5")
install.packages("caret")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("tree")
install.packages("randomForest")
install.packages("e1071")
install.packages("ggplot2")
install.packages("caTools")
##
## Seaflow assignment
## UW Data Science Series Assignment 5
## Practical Predictive Analytics Assignment 1
library(caret)
library(rpart)
library(tree)
library(randomForest)
library(e1071)
library(ggplot2)
## Step 1: Read and summarize the data
sf<-read.csv("seaflow_21min.csv")
str(sf)
summary(sf)
# Q2 How many particles labeled "synecho" are in the file provided?
nrow(sf[ sf$pop=="synecho", ])
table(sf$pop)
# Q3 What is the 3rd Quantile of the field fsc_small?
summary(sf)
## Step 2: Split the data into test and training sets
# Divide the data into two equal subsets, one for training and one for
# testing. Make sure to divide the data in an unbiased manner.
# You might consider using either the createDataPartition function or
# the sample function, although there are many ways to achieve the goal.
library(caTools)
set.seed(1000)
spl = sample.split(sf$time, SplitRatio = 0.5)
train = subset(sf, spl==TRUE)
test = subset(sf, spl==FALSE)
#Q4 What is the mean of the variable "time" for your training set?
mean(train$time)
## Step 3: Plot the data
# Plot pe against chl_small and color by pop
plot1<-ggplot(data=sf,aes(x=chl_small,y=pe,col=pop))+geom_point()
plot1
# Q5 In the plot of pe vs. chl_small, the particles labeled ultra
# should appear to be somewhat "mixed" with two other populations
# of particles. Which two populations?
## Step 4: Train a decision tree.
library(rpart)
library(rpart)
library(rpart.plot)
# construct formula object
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small)
CARTmodel <- rpart(fol, method="class", data=train)
print(CARTmodel)
# same thing, using AE terminology
CARTmodel = rpart(pop ~ fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small, data=train, method="class")
prp(CARTb)
## Step 5: Evaluate the decision tree on the test data.
#accuracy of CART model on test set
PredictCARTmodel = predict(CARTmodel, newdata = test, type = "class")
ct<-table(test$pop, PredictCARTmodel)
ct
sum(diag(ct))/sum(ct)
sum(PredictCARTmodel==test$pop)/nrow(test)
## Step 6: Build and evaluate a random forest.
# Install randomForest package
#install.packages("randomForest")
library(randomForest)
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe +chl_small)
forestmodel<-randomForest(fol, data=train)
PredictForest = predict(forestmodel, newdata = test)
sum(PPredictForest==test$pop)/nrow(test)
sum(PredictForest==test$pop)/nrow(test)
importance(forestmodel)
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe +chl_small)
SVMmodel <- svm(fol, data=train)
PredictSVM = predict(SVMmodel, newdata = test)
sum(PredictSVM==test$pop)/nrow(test)
table(pred = PredictCARTmodel, true = test$pop)
table(pred = PredictForest, true = test$pop)
table(pred = PredictSVM, true = test$pop)
plot(sf$time,sf$chl_small)
plot(sf$time,sf$chl_perp)
plot(sf$time,sf$fsc_small)
plot(sf$time,sf$fsc_perp)
plot(sf$time,sf$fsc_big)
plot(sf$time,sf$pe)
plot(sf$time,sf$chl_small)
plot(sf$time,sf$chl_big)
sfclean<-sf[ sf$file_id!=208, ]
summary(sfclean)
table(sfclean$file_id)
sfclean<-sf[ sf$file_id!=208, ]
table(sfclean$file_id) # check that all file_id 208 removed
# repeat analysis
library(caTools)
set.seed(1000)
spl = sample.split(sfclean$time, SplitRatio = 0.5)
train = subset(sfclean, spl==TRUE)
test = subset(sfclean, spl==FALSE)
# repeat analysis
library(caTools)
set.seed(1000)
spl = sample.split(sfclean$time, SplitRatio = 0.5)
train = subset(sfclean, spl==TRUE)
test = subset(sfclean, spl==FALSE)
# Decision tree (CART model)
library(rpart)
library(rpart)
library(rpart.plot)
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small)
CARTmodel <- rpart(fol, method="class", data=train)
print(CARTmodel)
PredictCARTmodel = predict(CARTmodel, newdata = test, type = "class")
sum(PredictCARTmodel==test$pop)/nrow(test)
# Random Forest model
library(randomForest)
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe +chl_small)
forestmodel<-randomForest(fol, data=train)
PredictForest = predict(forestmodel, newdata = test)
sum(PredictForest==test$pop)/nrow(test)
importance(forestmodel)
# SVM model
library(e1071)
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe +chl_small)
SVMmodel <- svm(fol, data=train)
PredictSVM = predict(SVMmodel, newdata = test)
sum(PredictSVM==test$pop)/nrow(test)
0.9252247-0.8828719
##
## Seaflow assignment
## UW Data Science Series Assignment 5
## Practical Predictive Analytics Assignment 1
library(caret)
library(rpart)
library(tree)
library(randomForest)
library(e1071)
library(ggplot2)
## Step 1: Read and summarize the data
sf<-read.csv("seaflow_21min.csv")
str(sf)
summary(sf)
# Q2 How many particles labeled "synecho" are in the file provided?
nrow(sf[ sf$pop=="synecho", ])
table(sf$pop)
# Q3 What is the 3rd Quantile of the field fsc_small?
summary(sf)
## Step 2: Split the data into test and training sets
# Divide the data into two equal subsets, one for training and one for
# testing. Make sure to divide the data in an unbiased manner.
# You might consider using either the createDataPartition function or
# the sample function, although there are many ways to achieve the goal.
library(caTools)
set.seed(1000)
spl = sample.split(sf$time, SplitRatio = 0.5)
train = subset(sf, spl==TRUE)
test = subset(sf, spl==FALSE)
#Q4 What is the mean of the variable "time" for your training set?
mean(train$time)
## Step 3: Plot the data
# Plot pe against chl_small and color by pop
plot1<-ggplot(data=sf,aes(x=chl_small,y=pe,col=pop))+geom_point()
plot1
# Q5 In the plot of pe vs. chl_small, the particles labeled ultra
# should appear to be somewhat "mixed" with two other populations
# of particles. Which two populations?
## Step 4: Train a decision tree.
library(rpart)
library(rpart)
library(rpart.plot)
# construct formula object
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small)
CARTmodel <- rpart(fol, method="class", data=train)
print(CARTmodel)
# same thing, using AE terminology
CARTmodel = rpart(pop ~ fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small, data=train, method="class")
prp(CARTb)
## Step 5: Evaluate the decision tree on the test data.
#accuracy of CART model on test set
PredictCARTmodel = predict(CARTmodel, newdata = test, type = "class")
ct<-table(test$pop, PredictCARTmodel)
ct
sum(diag(ct))/sum(ct)
sum(PredictCARTmodel==test$pop)/nrow(test)
## Step 6: Build and evaluate a random forest.
# Install randomForest package
#install.packages("randomForest")
library(randomForest)
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small)
forestmodel<-randomForest(fol, data=train)
PredictForest = predict(forestmodel, newdata = test)
sum(PredictForest==test$pop)/nrow(test)
importance(forestmodel)
## Step 7: Train a support vector machine model and compare results.
# Use the e1071 library and call model <- svm(fol, data=trainingdata).
library(e1071)
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small)
SVMmodel <- svm(fol, data=train)
PredictSVM = predict(SVMmodel, newdata = test)
sum(PredictSVM==test$pop)/nrow(test)
## Step 8: Construct confusion matrices
# CART
table(pred = PredictCARTmodel, true = test$pop)
#randomForest
table(pred = PredictForest, true = test$pop)
# SVM model
table(pred = PredictSVM, true = test$pop)
## Step 8: Sanity check the data
plot(sf$time,sf$fsc_small)
plot(sf$time,sf$fsc_perp)
plot(sf$time,sf$fsc_big)
plot(sf$time,sf$pe)
plot(sf$time,sf$chl_small)
plot(sf$time,sf$chl_big)
## filter out data connected with file_id 208
sfclean<-sf[ sf$file_id!=208, ]
table(sfclean$file_id) # check that all file_id 208 removed
# repeat analysis
library(caTools)
set.seed(1000)
spl = sample.split(sfclean$time, SplitRatio = 0.5)
train = subset(sfclean, spl==TRUE)
test = subset(sfclean, spl==FALSE)
# Decision tree (CART model)
library(rpart)
library(rpart)
library(rpart.plot)
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small)
CARTmodel <- rpart(fol, method="class", data=train)
print(CARTmodel)
PredictCARTmodel = predict(CARTmodel, newdata = test, type = "class")
sum(PredictCARTmodel==test$pop)/nrow(test)
# Random Forest model
library(randomForest)
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small)
forestmodel<-randomForest(fol, data=train)
PredictForest = predict(forestmodel, newdata = test)
sum(PredictForest==test$pop)/nrow(test)
importance(forestmodel)
# SVM model
library(e1071)
fol<-formula(pop ~ fsc_small + fsc_perp + fsc_big + pe + chl_big + chl_small)
SVMmodel <- svm(fol, data=train)
PredictSVM = predict(SVMmodel, newdata = test)
sum(PredictSVM==test$pop)/nrow(test)
0.9720131-0.9200985
